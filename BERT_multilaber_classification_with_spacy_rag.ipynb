{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10ivw14El60DfnST0P47J9H37dev9kFJY",
      "authorship_tag": "ABX9TyPT0tLW5dI9u6+p+QhvFeVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MF884/Machine-Learning-Projects/blob/main/BERT_multilaber_classification_with_spacy_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from annoy import AnnoyIndex\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16 if torch.cuda.is_available() else 8\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-5\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.sentiment_keywords = {\n",
        "            'positive': {'thanks': 2, 'great': 2, 'solved': 3, 'love': 2, 'ðŸ˜Š': 3},\n",
        "            'negative': {'ðŸ˜¡': 3, 'sucks': 3, 'hate': 3, 'broken': 2, 'painful': 2}\n",
        "        }\n",
        "        self.issue_keywords = {\n",
        "            'technical': ['broken', 'crash', 'freezes', 'bug', 'error'],\n",
        "            'service': ['delay', 'refund', 'callback', 'support'],\n",
        "            'performance': ['battery', 'slow', 'speed', 'drains']\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        doc = nlp(text)\n",
        "        return \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
        "\n",
        "    def load_and_validate_data(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['text'] = df['text'].str.replace(r'http\\S+', '', regex=True)\n",
        "        df = df.dropna(subset=['text'])\n",
        "        df['text'] = df['text'].apply(self.preprocess_text)\n",
        "        df['labels'] = df['text'].apply(self.generate_labels)\n",
        "        assert len(df['text']) == len(df['labels']), \"Text and labels length mismatch\"\n",
        "        return df\n",
        "\n",
        "    def generate_labels(self, text):\n",
        "        text_lower = text.lower()\n",
        "        labels = []\n",
        "        pos_score = sum(weight for word, weight in self.sentiment_keywords['positive'].items() if word in text_lower)\n",
        "        neg_score = sum(weight for word, weight in self.sentiment_keywords['negative'].items() if word in text_lower)\n",
        "        if pos_score > neg_score:\n",
        "            labels.append('positive')\n",
        "        elif neg_score > pos_score:\n",
        "            labels.append('negative')\n",
        "        else:\n",
        "            labels.append('neutral')\n",
        "        for issue_type, keywords in self.issue_keywords.items():\n",
        "            if any(keyword in text_lower for keyword in keywords):\n",
        "                labels.append(issue_type)\n",
        "        return labels\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts.reset_index(drop=True)\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text, max_length=self.max_len, truncation=True, padding='max_length', return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "def build_rag_index(texts):\n",
        "    vector_dim = 768\n",
        "    annoy_index = AnnoyIndex(vector_dim, 'angular')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('roberta-base').to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_LEN).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = model(**inputs).logits.cpu().numpy().flatten()\n",
        "        annoy_index.add_item(i, embedding)\n",
        "\n",
        "    annoy_index.build(10)  # Build index with 10 trees\n",
        "    return annoy_index\n",
        "\n",
        "def safe_train_test_split(texts, labels, test_size=0.2):\n",
        "    return train_test_split(texts, labels, test_size=test_size, random_state=42, stratify=None)\n",
        "\n",
        "def main():\n",
        "    processor = DataProcessor()\n",
        "    df = processor.load_and_validate_data('sample.csv')\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    y = mlb.fit_transform(df['labels'])\n",
        "    X_train, X_test, y_train, y_test = safe_train_test_split(df['text'], y)\n",
        "    X_train, X_val, y_train, y_val = safe_train_test_split(X_train, y_train, test_size=0.25)\n",
        "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "    train_dataset = TweetDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "    val_dataset = TweetDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
        "    test_dataset = TweetDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        'roberta-base', num_labels=len(mlb.classes_), problem_type=\"multi_label_classification\"\n",
        "    ).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "    rag_index = build_rag_index(df['text'])\n",
        "    print(\"RAG Index built successfully\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            optimizer.zero_grad()\n",
        "            inputs = {key: batch[key].to(device) for key in ['input_ids', 'attention_mask', 'labels']}\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fhjW9lmO6QF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}