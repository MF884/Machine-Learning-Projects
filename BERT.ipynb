{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN82qHj0Uwkk76l6TCvYk9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MF884/Machine-Learning-Projects/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uew9YR0l5A4-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW  # Correct import location\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 6. Training setup\n",
        "EPOCHS = 5\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)  # This line stays the same, just the import changed\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16 if torch.cuda.is_available() else 8\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-5\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# 1. Enhanced Data Processor with Shape Validation\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.sentiment_keywords = {\n",
        "            'positive': {'thanks': 2, 'great': 2, 'solved': 3, 'love': 2, 'ðŸ˜Š': 3},\n",
        "            'negative': {'ðŸ˜¡': 3, 'sucks': 3, 'hate': 3, 'broken': 2, 'painful': 2}\n",
        "        }\n",
        "        self.issue_keywords = {\n",
        "            'technical': ['broken', 'crash', 'freezes', 'bug', 'error'],\n",
        "            'service': ['delay', 'refund', 'callback', 'support'],\n",
        "            'performance': ['battery', 'slow', 'speed', 'drains']\n",
        "        }\n",
        "\n",
        "    def load_and_validate_data(self, file_path):\n",
        "        \"\"\"Load data and validate shapes\"\"\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['text'] = df['text'].str.replace(r'http\\S+', '', regex=True)\n",
        "        df = df.dropna(subset=['text'])  # Remove empty texts\n",
        "\n",
        "        # Generate labels\n",
        "        df['labels'] = df['text'].apply(self.generate_labels)\n",
        "\n",
        "        # Verify all samples got labels\n",
        "        assert len(df['text']) == len(df['labels']), \"Text and labels length mismatch\"\n",
        "\n",
        "        return df\n",
        "\n",
        "    def generate_labels(self, text):\n",
        "        \"\"\"Ensure consistent label format\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        labels = []\n",
        "\n",
        "        # Sentiment\n",
        "        pos_score = sum(weight for word, weight in self.sentiment_keywords['positive'].items()\n",
        "                       if word in text_lower)\n",
        "        neg_score = sum(weight for word, weight in self.sentiment_keywords['negative'].items()\n",
        "                       if word in text_lower)\n",
        "\n",
        "        if pos_score > neg_score:\n",
        "            labels.append('positive')\n",
        "        elif neg_score > pos_score:\n",
        "            labels.append('negative')\n",
        "        else:\n",
        "            labels.append('neutral')\n",
        "\n",
        "        # Issues\n",
        "        for issue_type, keywords in self.issue_keywords.items():\n",
        "            if any(keyword in text_lower for keyword in keywords):\n",
        "                labels.append(issue_type)\n",
        "\n",
        "        return labels\n",
        "\n",
        "# 2. Dataset Class with Shape Validation\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        assert len(texts) == len(labels), f\"Texts ({len(texts)}) and labels ({len(labels)}) length mismatch\"\n",
        "        self.texts = texts.reset_index(drop=True)\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "# 3. Model Training with Safe Splitting\n",
        "def safe_train_test_split(texts, labels, test_size=0.2):\n",
        "    \"\"\"Guaranteed consistent splitting\"\"\"\n",
        "    assert len(texts) == len(labels), f\"Mismatched lengths: texts {len(texts)}, labels {len(labels)}\"\n",
        "    return train_test_split(\n",
        "        texts,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=None  # Removing stratification to ensure it works\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    # Initialize components\n",
        "    processor = DataProcessor()\n",
        "\n",
        "    # Load and validate data\n",
        "    try:\n",
        "        df = processor.load_and_validate_data('/content/drive/My Drive/Colab Notebooks/sample.csv')\n",
        "    except Exception as e:\n",
        "        print(f\"Data loading failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Prepare labels\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    y = mlb.fit_transform(df['labels'])\n",
        "    print(f\"Data shapes - X: {len(df)}, y: {len(y)}\")\n",
        "\n",
        "    # Safe splitting\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = safe_train_test_split(df['text'], y)\n",
        "        X_train, X_val, y_train, y_val = safe_train_test_split(X_train, y_train, test_size=0.25)\n",
        "        print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"Splitting failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Initialize tokenizer and datasets\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    try:\n",
        "        train_dataset = TweetDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "        val_dataset = TweetDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
        "        test_dataset = TweetDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
        "    except AssertionError as e:\n",
        "        print(f\"Dataset creation failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Model setup\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=len(mlb.classes_),\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device),\n",
        "                'labels': batch['labels'].to(device)\n",
        "            }\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs = {\n",
        "                    'input_ids': batch['input_ids'].to(device),\n",
        "                    'attention_mask': batch['attention_mask'].to(device),\n",
        "                    'labels': batch['labels'].to(device)\n",
        "                }\n",
        "                outputs = model(**inputs)\n",
        "                val_loss += loss_fn(outputs.logits, inputs['labels']).item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device)\n",
        "            }\n",
        "            labels = batch['labels'].numpy()\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            preds = (torch.sigmoid(outputs.logits) > THRESHOLD).int().cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=mlb.classes_, zero_division=0))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}